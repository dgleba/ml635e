{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dg chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- -----------\n",
      "absl-py                       1.1.0\n",
      "aiohttp                       3.8.1\n",
      "aiosignal                     1.2.0\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "astunparse                    1.6.3\n",
      "async-timeout                 4.0.2\n",
      "attrs                         21.4.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "beautifulsoup4                4.11.1\n",
      "bleach                        5.0.0\n",
      "blinker                       1.4\n",
      "brotlipy                      0.7.0\n",
      "cached-property               1.5.2\n",
      "cachetools                    4.2.4\n",
      "certifi                       2022.5.18.1\n",
      "cffi                          1.15.0\n",
      "charset-normalizer            2.0.12\n",
      "click                         8.1.3\n",
      "cryptography                  37.0.2\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.30\n",
      "debugpy                       1.6.0\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "detecto                       1.2.2\n",
      "entrypoints                   0.4\n",
      "fastjsonschema                2.15.3\n",
      "filelock                      3.7.1\n",
      "flatbuffers                   1.12\n",
      "flit_core                     3.7.1\n",
      "fonttools                     4.33.3\n",
      "frozenlist                    1.3.0\n",
      "gast                          0.4.0\n",
      "gdown                         4.4.0\n",
      "google-auth                   1.35.0\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-pasta                  0.2.0\n",
      "grpcio                        1.43.0\n",
      "h5py                          3.6.0\n",
      "idna                          3.3\n",
      "importlib-metadata            4.11.3\n",
      "importlib-resources           5.7.1\n",
      "ipykernel                     6.4.2\n",
      "ipython                       7.33.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.7.0\n",
      "jedi                          0.18.1\n",
      "Jinja2                        3.1.2\n",
      "joblib                        1.1.0\n",
      "jsonschema                    4.5.1\n",
      "jupyter-client                7.3.0\n",
      "jupyter-core                  4.9.2\n",
      "jupyterlab-pygments           0.2.2\n",
      "jupyterlab-widgets            1.1.0\n",
      "keras                         2.9.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "keras-resnet                  0.2.0\n",
      "keras-retinanet               1.0.0\n",
      "kiwisolver                    1.4.2\n",
      "lfm                           3.1\n",
      "libclang                      14.0.1\n",
      "Markdown                      3.3.7\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.5.2\n",
      "matplotlib-inline             0.1.3\n",
      "mistune                       0.8.4\n",
      "multidict                     6.0.2\n",
      "nbclient                      0.6.2\n",
      "nbconvert                     6.5.0\n",
      "nbformat                      5.4.0\n",
      "nest-asyncio                  1.5.5\n",
      "notebook                      6.4.11\n",
      "numpy                         1.22.3\n",
      "oauthlib                      3.2.0\n",
      "opencv-python                 4.6.0.66\n",
      "opt-einsum                    3.3.0\n",
      "packaging                     21.3\n",
      "pandas                        1.4.2\n",
      "pandocfilters                 1.5.0\n",
      "parso                         0.8.3\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.1.1\n",
      "pip                           21.2.4\n",
      "progressbar2                  4.0.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.29\n",
      "protobuf                      3.19.4\n",
      "ptyprocess                    0.7.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.7\n",
      "pycparser                     2.21\n",
      "Pygments                      2.12.0\n",
      "PyJWT                         2.4.0\n",
      "pyOpenSSL                     22.0.0\n",
      "pyparsing                     3.0.8\n",
      "pyrsistent                    0.18.1\n",
      "PySocks                       1.7.1\n",
      "python-dateutil               2.8.2\n",
      "python-utils                  3.3.3\n",
      "pytz                          2022.1\n",
      "pyu2f                         0.1.5\n",
      "pyzmq                         22.3.0\n",
      "requests                      2.27.1\n",
      "requests-oauthlib             1.3.1\n",
      "rsa                           4.8\n",
      "scikit-learn                  1.1.1\n",
      "scipy                         1.8.1\n",
      "seaborn                       0.11.2\n",
      "Send2Trash                    1.8.0\n",
      "setuptools                    62.1.0\n",
      "simple-image-download         0.2\n",
      "six                           1.16.0\n",
      "sklearn                       0.0\n",
      "soupsieve                     2.3.1\n",
      "tensorboard                   2.9.1\n",
      "tensorboard-data-server       0.6.0\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorflow                    2.7.1\n",
      "tensorflow-estimator          2.9.0\n",
      "tensorflow-gpu                2.9.1\n",
      "tensorflow-io-gcs-filesystem  0.26.0\n",
      "termcolor                     1.1.0\n",
      "terminado                     0.13.3\n",
      "threadpoolctl                 3.1.0\n",
      "tinycss2                      1.1.1\n",
      "torch                         1.11.0\n",
      "torchvision                   0.12.0\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.0\n",
      "traitlets                     5.1.1\n",
      "typing_extensions             4.2.0\n",
      "urllib3                       1.26.9\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "Werkzeug                      2.1.2\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            3.6.0\n",
      "wrapt                         1.14.1\n",
      "yarl                          1.7.2\n",
      "zipp                          3.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ds9xP7rtnrZ"
   },
   "source": [
    "Next, change directory to wherever you created your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FkyXFsLGtvyc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Change this to your Drive folder location\n",
    "WORKING_DIRECTORY = '/home/studio-lab-user/ml635e/ir'\n",
    "\n",
    "os.chdir(WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1654738344918,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "nNbnk4pitjF0",
    "outputId": "a9ad9d39-1104-4098-fda5-f31c570f58be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_this.12a.txt  images  train  val\n"
     ]
    }
   ],
   "source": [
    "# List the contents of your working directory\n",
    "# It should contain at least three folders: images, train_labels, and val_labels\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43sPuLgxuemn"
   },
   "source": [
    "Now, let's install the Detecto package using pip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3427,
     "status": "ok",
     "timestamp": 1654738351538,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "M3-sRIuiuYDh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b97165ba-2a42-4df2-8c2d-982be162c3fa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: detecto in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from detecto) (4.64.0)\n",
      "Requirement already satisfied: torchvision in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from detecto) (0.12.0)\n",
      "Requirement already satisfied: matplotlib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from detecto) (3.5.2)\n",
      "Requirement already satisfied: opencv-python in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from detecto) (4.6.0.66)\n",
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from detecto) (1.4.2)\n",
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from detecto) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (1.22.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (3.0.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (9.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->detecto) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->detecto) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas->detecto) (2022.1)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch->detecto) (4.2.0)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchvision->detecto) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision->detecto) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision->detecto) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision->detecto) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision->detecto) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "# Note: if it states you must restart the runtime in order to use a\n",
    "# newly installed version of a package, you do NOT need to do this. \n",
    "!pip install detecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peLysju5vOgA"
   },
   "source": [
    "Import everything we need in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XgJi8407uowH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from detecto import core, utils, visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5UqkoIevXSX"
   },
   "source": [
    "To check that everything's working, we can try reading in one of the images from our images folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1654738365701,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "FmjpizSMvJLn",
    "outputId": "d514d7dc-8752-4832-adbd-741915861fc0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = utils.read_image('images/inner_rim_210805T104053.png')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCEC_0EQwLDf"
   },
   "source": [
    "How cute! Now, we're ready to create our dataset and train our model. However, before doing so, it's a bit slow working with hundreds of individual XML label files, so we should convert them into a single CSV file to save time later down the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 12135,
     "status": "ok",
     "timestamp": 1654738743456,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "_Pnr8quRv8v7",
    "outputId": "53a5f852-6dbe-4e3f-c754-6ea474f9bb2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inner_rim_210805T104053.png</td>\n",
       "      <td>260</td>\n",
       "      <td>7990</td>\n",
       "      <td>Chip</td>\n",
       "      <td>2</td>\n",
       "      <td>4899</td>\n",
       "      <td>192</td>\n",
       "      <td>5231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inner_rim_210805T104112.png</td>\n",
       "      <td>260</td>\n",
       "      <td>7990</td>\n",
       "      <td>Chip</td>\n",
       "      <td>1</td>\n",
       "      <td>4013</td>\n",
       "      <td>192</td>\n",
       "      <td>4332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inner_rim_210805T104131.png</td>\n",
       "      <td>260</td>\n",
       "      <td>7990</td>\n",
       "      <td>Chip</td>\n",
       "      <td>1</td>\n",
       "      <td>1364</td>\n",
       "      <td>191</td>\n",
       "      <td>1694</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  width  height class  xmin  ymin  xmax  ymax  \\\n",
       "0  inner_rim_210805T104053.png    260    7990  Chip     2  4899   192  5231   \n",
       "1  inner_rim_210805T104112.png    260    7990  Chip     1  4013   192  4332   \n",
       "2  inner_rim_210805T104131.png    260    7990  Chip     1  1364   191  1694   \n",
       "\n",
       "   image_id  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do this twice: once for our trning labels and once for our validation labels\n",
    "utils.xml_to_csv('train', 'train.csv')\n",
    "utils.xml_to_csv('val', 'val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csLF8GKQ3CGe"
   },
   "source": [
    "Below, we create our dataset, applying a couple of transforms beforehand. These are optional, but they can be useful for augmenting your dataset without gathering more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1654741423315,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "m7j17jxf31Gk",
    "outputId": "2f47347d-66ca-485b-f655-216806d8ae4c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify a list of transformations for our dataset to apply on our images\n",
    "# transform_img = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize(800),\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.ToTensor(),\n",
    "#     utils.normalize_transform(),\n",
    "# ])\n",
    "\n",
    "dataset = core.Dataset('train.csv', 'images/')\n",
    "\n",
    "# dataset[i] returns a tuple containing our transformed image and\n",
    "# and a dictionary containing label and box data\n",
    "image, target = dataset[0]\n",
    "\n",
    "# Show our image along with the box. Note: it may\n",
    "# be colored oddly due to being normalized by the \n",
    "# dataset and then reverse-normalized for plotting\n",
    "visualize.show_labeled_image(image, target['boxes'], target['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPFYk-lv6Cgd"
   },
   "source": [
    "Finally, let's train our model! First, we create a DataLoader over our dataset to specify how we feed the images into our model. We also use our validation dataset to track the accuracy of the model throughout training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 50319,
     "status": "error",
     "timestamp": 1654741490003,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "WLRHugVo6kAt",
    "outputId": "a9c7a102-cfe6-40eb-ed6e-64d5a0e3c02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're training your model on a CPU. Consider switching to a GPU; otherwise, this method can take hours upon hours or even days to finish. For more information, see https://detecto.readthedocs.io/en/latest/usage/quickstart.html#technical-requirements\n",
      "Epoch 1 of 3\n",
      "Begin iterating over training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77/1923109836.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the model! This step can take a while, so make sure you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# the GPU is turned on in Edit -> Notebook settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Plot the accuracy over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/detecto/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, val_dataset, epochs, learning_rate, momentum, weight_decay, gamma, lr_step_size, verbose)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;31m# Calculate the model's loss (i.e. how well it does on the current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;31m# image and target, with a lower loss being better)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m                 \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_roi_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mx_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatmap_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_levels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             self.scales, self.map_levels = _setup_scales(\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mx_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m_setup_scales\u001b[0;34m(features, image_shapes, canonical_scale, canonical_level)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0moriginal_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_infer_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_input_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;31m# get the levels in the feature map by leveraging the fact that the network always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# downsamples by a factor of 2 at each level.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0moriginal_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_infer_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_input_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;31m# get the levels in the feature map by leveraging the fact that the network always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# downsamples by a factor of 2 at each level.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m_infer_scale\u001b[0;34m(feature, original_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mpossible_scales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mpossible_scales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpossible_scales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpossible_scales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create our validation dataset\n",
    "val_dataset = core.Dataset('val.csv', 'images/')\n",
    "\n",
    "# Create the loader for our training dataset\n",
    "loader = core.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Create our model, passing in all unique classes we're predicting\n",
    "# Note: make sure these match exactly with the labels in the XML/CSV files!\n",
    "model = core.Model(['Chip'])\n",
    "\n",
    "# Train the model! This step can take a while, so make sure you\n",
    "# the GPU is turned on in Edit -> Notebook settings\n",
    "losses = model.fit(loader, val_dataset, epochs=3, verbose=True)\n",
    "\n",
    "# Plot the accuracy over time\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeRKZMNJ9cfc"
   },
   "source": [
    "Let's see how well our model does on a couple images from our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "executionInfo": {
     "elapsed": 3480,
     "status": "ok",
     "timestamp": 1654739642859,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "rvHbAcLb9cIL",
    "outputId": "b3187aa0-913e-49f5-e767-858387ef12e6"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "# Create a list of images  from val_dataset\n",
    "for i in range(0, 2, 1):\n",
    "    image, _ = val_dataset[i]\n",
    "    images.append(image)\n",
    "\n",
    "# Plot a  grid of the model's predictions on our  images\n",
    "visualize.plot_prediction_grid(model, images, dim=(1, 3), figsize=(33, 33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDz1v5Lh-uZG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Thanks for making it this far through the demo!\n",
    "\n",
    "This is as far as the demo goes, but a great next step would be seeing how well the model works on a live video of Chihuahuas and Golden Retrievers in the same frame at the same time. To learn more about Detecto, be sure to check out the [Quickstart guide](https://detecto.readthedocs.io/en/latest/usage/quickstart.html), [Further Usage guide](https://detecto.readthedocs.io/en/latest/usage/further-usage.html), and [API docs](https://detecto.readthedocs.io/en/latest/api.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PgWhg2B0XhK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "detecto-dg01.ipynb",
   "provenance": [
    {
     "file_id": "1ISaTV5F-7b4i2QqtjTa7ToDPQ2k8qEe0",
     "timestamp": 1654741560702
    }
   ]
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
